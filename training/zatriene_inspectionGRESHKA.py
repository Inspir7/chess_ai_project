import torch
from training.RL_pipeline import RLPipeline

# --- –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("üñ•Ô∏è Device:", device)

# --- –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ RL pipeline ---
pipeline = RLPipeline(device=device)
model = pipeline.model
buffer = pipeline.buffer
model.eval()

print("‚úÖ –ú–æ–¥–µ–ª—ä—Ç –µ —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω –æ—Ç RL Pipeline!")

# --- –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ self-play ---
print("\nüéÆ –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ self-play –µ–ø–∏–∑–æ–¥...")
examples = pipeline.self_play_episode(mcts_sims=30)
print(f"‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–∏ {len(examples)} –ø–æ–∑–∏—Ü–∏–∏ –æ—Ç self-play")

# --- –î–æ–±–∞–≤—è–Ω–µ –≤ –±—É—Ñ–µ—Ä–∞ ---
pipeline.add_examples(examples)
print(f"üì¶ Buffer size: {len(buffer)}")

# --- –í–∑–µ–º–∞–º–µ –±–∞—Ç—á –∑–∞ forward pass ---
batch_size = min(8, len(buffer))
states, policies, values = buffer.sample(batch_size)
states, policies, values = states.to(device), policies.to(device), values.to(device)

# --- Forward pass ---
with torch.no_grad():
    pred_policy, pred_value = model(states)

# --- –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–∞ –∑–∞–≥—É–±–∏—Ç–µ –∏ outputs ---
policy_loss = torch.nn.functional.cross_entropy(pred_policy, torch.argmax(policies, dim=1))
value_loss = torch.nn.functional.mse_loss(pred_value, values)

print("\nüìä Forward pass diagnostics:")
print(f"Policy loss: {policy_loss.item():.4f}")
print(f"Value loss: {value_loss.item():.4f}")
print(f"Policy output min/max: {pred_policy.min().item():.4f}/{pred_policy.max().item():.4f}")
print(f"Value output min/max: {pred_value.min().item():.4f}/{pred_value.max().item():.4f}")

# --- –û–ø—Ü–∏–æ–Ω–∞–ª–Ω–æ: –ø–µ—á–∞—Ç –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ–Ω —Ö–æ–¥ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç ---
example_pi = examples[0][1]
action = torch.argmax(example_pi).item()
print(f"\nüéØ –ü—Ä–∏–º–µ—Ä–µ–Ω move index: {action}, max prob: {example_pi.max().item():.4f}")
